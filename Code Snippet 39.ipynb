{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data\n",
      "+--------+--------------------+\n",
      "|category|             content|\n",
      "+--------+--------------------+\n",
      "|     ham|Meet me at Willys...|\n",
      "|     ham|Let us go to the ...|\n",
      "|    spam|Free entry in 2 a...|\n",
      "|     ham|I have sent you t...|\n",
      "|     ham|Lets meet at 7pm ...|\n",
      "+--------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Final Data\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(13497,[73,82,940...|  0.0|\n",
      "|(13497,[7,85,127,...|  0.0|\n",
      "|(13497,[2,13,19,2...|  1.0|\n",
      "|(13497,[95,472,75...|  0.0|\n",
      "|(13497,[73,491,34...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Predictions\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(13497,[0,1,2,4,3...|  1.0|[-26.086348683110...|[4.68643625282683...|       1.0|\n",
      "|(13497,[0,1,2,5,5...|  1.0|[37.1275031341671...|[1.0,7.5115620994...|       0.0|\n",
      "|(13497,[0,1,2,7,8...|  0.0|[74.6484871191991...|[1.0,3.8069218514...|       0.0|\n",
      "|(13497,[0,1,2,12,...|  1.0|[-24.926347946875...|[1.49494299554241...|       1.0|\n",
      "|(13497,[0,1,3,9,1...|  0.0|[17.0246781291098...|[0.99999995960978...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "The Accuracy is 0.9714649365856715\n"
     ]
    }
   ],
   "source": [
    "#Code Snippet 39\n",
    "#Step 1 - Importing data and necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SpamHamNLP').getOrCreate()\n",
    "data = spark.read.csv('spam_ham_nlp.csv',header=True,inferSchema=True,sep='\\t')\n",
    "print(\"Initial Data\")\n",
    "data.show(5)\n",
    "#Step 2 - Data pre-processing and applying NLP data format techniques\n",
    "from pyspark.ml.feature import (StringIndexer,Tokenizer, StopWordsRemover,CountVectorizer,IDF,VectorAssembler)                         \n",
    "#Converting our category into a numeric\n",
    "category_to_numeric = StringIndexer(inputCol='category',outputCol='label')\n",
    "#Tokenizing our content\n",
    "tokenizer = Tokenizer(inputCol='content',outputCol='tokens')\n",
    "#Removing the stop words \n",
    "stopWords_removed = StopWordsRemover(inputCol='tokens',outputCol='stpWrd_tokens')\n",
    "#Converting tokens to vectors of token count\n",
    "count_vectors = CountVectorizer(inputCol='stpWrd_tokens',outputCol='countVec')\n",
    "#Performing IDF \n",
    "idf = IDF(inputCol='countVec',outputCol='tf-idf')\n",
    "#consolidating the features\n",
    "consolidated_data = VectorAssembler(inputCols=['tf-idf'],outputCol='features')\n",
    "#Transforming and finalizing our data to spark accepted format\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline_object = Pipeline(stages=[category_to_numeric,tokenizer,stopWords_removed,count_vectors,idf,consolidated_data])                        \n",
    "pipeline_data_model = pipeline_object.fit(data)\n",
    "final_data = pipeline_data_model.transform(data)\n",
    "final_data.head(1)\n",
    "final_data = final_data.select('features','label')\n",
    "print('Final Data')\n",
    "final_data.show(5)\n",
    "#Step 3 - Applying Machine learning algorithm to our data\n",
    "#Using a Logistic Regression Classifier as our classification algorithm\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "# Splitting the data into 70 and 30 percent\n",
    "train_data, test_data = final_data.randomSplit([0.7,0.3])\n",
    "spam_detector = log_reg.fit(train_data)\n",
    "predictions = spam_detector.transform(test_data)\n",
    "print(\"Predictions\")\n",
    "predictions.show(5)\n",
    "#Step 4 - Evaluating our Model\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "eval_object = MulticlassClassificationEvaluator()\n",
    "accuracy = eval_object.evaluate(predictions)\n",
    "print(\"The Accuracy is {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
